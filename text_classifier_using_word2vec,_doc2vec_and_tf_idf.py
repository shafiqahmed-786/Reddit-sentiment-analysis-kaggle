# -*- coding: utf-8 -*-
"""Text Classifier Using Word2Vec, Doc2Vec and TF-IDF

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/shafiqahmed786/text-classifier-using-word2vec-doc2vec-and-tf-idf.f32b0ed8-b933-44b7-9d5f-0038aa5a37bc.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251006/auto/storage/goog4_request%26X-Goog-Date%3D20251006T074125Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7bdf5c4fec548fab2f5461040bcaaceffb1156d8ee508bacf4fc3f51e678500bf60fd044b5c0e3f7f6f0a89ba7ae081dac21b98f1ac5db4625b7d0726b2e219eda3643437822362e49ec3b1969f30e85330551ccb4131402930e3f444e3d37d92fce4d18888b062d8313f8faabeab1f9c4bb4b909e3abee3b5044796b0e38ac9256d070a65e9f404f316d19a2cbada9f6accec8e15f0aed94c12a99061f17fb0919cf1bd21476c81e2455c63aad77fe5fadc316944fe6de46d119252dbb9559396c1ce1edba9ffbc90e3c001959a352a68a753b20bfe2c71fa24cb655379bf038b69322e8b37b0a69fbbfb4730016b98add236d122d73d1e2991ebbc637b3133

# Introduction

In this notebook we shall perform Text Classification on Reddit Dataset using NLP Techniques.

The outline is as follows:  
      1. We shall perform Data Cleaning using various libraries in Python.  
      2. We shall Convert the Text into Vectors by using Algorithms including TF-IDF, WORD2VEC, DOC2VEC  
      3. Now, we shall make use of Classical Machine Learning Algorithms for Text Classification  

*Throughout this notebook, I have tried to explain all my steps using comments in each cell for easier understanding*

**Please do upvote if you like my work :)**

***

**SO LET'S START!**

# **FIRST, LET'S LOAD OUR DATA**
"""

import numpy as np
import pandas as pd

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

!pip install textacy

# IMPORTING LIBRARIES

import pandas as pd
import numpy as np
import emoji
from textacy import preprocessing
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.stem.snowball import SnowballStemmer
from gensim.models import Word2Vec
import gensim

!pip install gensim

!pip install emoji

# READING THE DATA

data = pd.read_pickle("/kaggle/input/reddit-comments-from-subreddits-humour-and-news/redditDataset.pkl")

# REVIEW DATA

data.head()

data.shape

data.info()

"""# TEXT PREPARATION"""

# CONVERTING TWEET TO LOWERCASE

data["text"] = data["text"].str.lower()

# REMOVING STOPWORDS FROM TWEETS

stop = stopwords.words('english')

#data["text"] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
data["text"] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

import nltk
nltk.download('stopwords')

# CONVERTING ALL EMOJIS TO TEXT

data["text"] = data["text"].apply(lambda x: emoji.demojize(x))

# REMOVING PUNCTUATION AND EXTRA WHITESPACE

preproc = preprocessing.make_pipeline(
    preprocessing.remove.punctuation,
    preprocessing.normalize.whitespace,
    preprocessing.replace.hashtags,
    preprocessing.remove.html_tags
 )

data["text"] = data["text"].apply(preproc)

# STEMMING THE WORDS

stemmer = SnowballStemmer("english")
data['text'] = data['text'].apply(lambda x: stemmer.stem(x))

data.head()

# SEPERATING INTO DEPENDENT AND INDEPENDENT VARIABLES

x = data[["text"]]
y = data["subreddit"]

# SEPERATING Y_TRAIN, Y_TEST; TO BE USED WHILE DOING MACHINE LEARNING

y_train = data["subreddit"].iloc[:80000]
y_test = data["subreddit"].iloc[80000:]

"""# TEXT VECTORIZATION

**Word2Vec Embeddings**
"""

# TOKENIZING THE TEXT BEFORE APPLYING WORD2VEC

tokenized_text = x['text'].apply(lambda x: x.split())
tokenized_text

# CREATING AN INSTANCE OF WORD2VEC

model_w2v = gensim.models.Word2Vec(
            tokenized_text,
            #size=200, # desired no. of features/independent variables
            window=5, # context window size
            min_count=2, # Ignores all words with total frequency lower than 2.
            sg = 1, # 1 for skip-gram model
            hs = 0,
            negative = 10, # for negative sampling
            workers= 32, # no.of cores
            seed = 34
)

# TRAINING WORD2VEC ON OUR THE TOKENIZED TEXT

model_w2v.train(tokenized_text, total_examples= len(x['text']), epochs=2)

# CHECKING OUR WORD2VEC MODEL

model_w2v.wv.most_similar(positive="dinner")

# CHECKING DIMENSION OF VECTOR OF THE WORDS AS CREATED BY MODEL

len(model_w2v.wv['food'])

# TAKING MEAN OF ALL VECTORS PRESENT IN WORD2VEC

def word_vector(tokens, size):
    vec = np.zeros(size).reshape((1, size))
    count = 0
    for word in tokens:
        try:
            vec += model_w2v.wv[word].reshape((1, size))
            count += 1.
        except KeyError:  # handling the case where the token is not in vocabulary
            continue
    if count != 0:
        vec /= count
    return vec

# PREPARING THE FEATURE SET FOR WORD2VEC

wordvec_arrays = np.zeros((len(tokenized_text), 100))
for i in range(len(tokenized_text)):
    wordvec_arrays[i,:] = word_vector(tokenized_text[i], 100)
wordvec_df = pd.DataFrame(wordvec_arrays)
wordvec_df.shape

# CREATING X_TRAIN, X_TEST BASED ON W2VEC EMBEDDINGS

x_train_w2v = wordvec_arrays[:80000,:]

x_test_w2v = wordvec_arrays[80000: ,]

"""**TF-IDF Vectorization**"""

# CONVERTING TEXT TO VECTORS USING TF-IDF ALGORITHM

tf = TfidfVectorizer()
x_tf = tf.fit_transform(x['text'])

x_train_tf = x_tf[:80000]
x_test_tf = x_tf[80000:]

"""**DOC2VEC EMBEDDINGS**"""

from gensim.test.utils import common_texts
from gensim.models.doc2vec import Doc2Vec, TaggedDocument

from tqdm import tqdm
tqdm.pandas(desc="progress-bar")

# TAGGING THE DOCUMENTS

def add_tag(twt):
    output = []
    for i, s in zip(twt.index, twt):
        output.append(TaggedDocument(s, ["tweet_" + str(i)]))
    return output

documents = add_tag(tokenized_text) # label all the tweets

# CHECKING A TAGGED DOCUMENT

documents[0]

# CREATING INSTANCE FOR DOC2VEC

model_d2v = gensim.models.Doc2Vec(dm=1, # dm = 1 for â€˜distributed memoryâ€™ model
                                  dm_mean=1, # dm_mean = 1 for using mean of the context word vector
                                  window=5, # width of the context window
                                  negative=7, # if > 0 then negative sampling will be used
                                  min_count=5, # Ignores all words with total frequency lower than 5.
                                  workers=32, # no. of cores
                                  alpha=0.1, # learning rate

                                 )

# BUILDING THE VOCAB

model_d2v.build_vocab([i for i in tqdm(documents)])

# TRAINING DOC2VEC ON OUR VOCAB

model_d2v.train(documents, total_examples= len(x['text']), epochs=2)

# CREATING FEATURE SET FOR DEC2VEC

docvec_arrays = np.zeros((len(tokenized_text), 100))

for i in range(len(x)):
    docvec_arrays[i,:] = model_d2v.dv[i].reshape((1,100))

docvec_df = pd.DataFrame(docvec_arrays)
docvec_df.shape

x_train_d2v = docvec_df.iloc[:80000]
x_test_d2v = docvec_df.iloc[80000:]

y_train_xg = y_train.replace({"humor":1, "news":0})
y_test_xg = y_test.replace({"humor":1, "news":0})

"""# FITTING MACHINE LEARNING MODELS

**1)RANDOM FOREST MODEL**
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score

# PREDICTION USING WORD2VEC EMBEDDINGS

rf_w2v = RandomForestClassifier()
rf_w2v.fit(x_train_w2v,y_train)

rf_pred_w2v = rf_w2v.predict(x_test_w2v)
accuracy_score(rf_pred_w2v, y_test)

# PREDICTION USING TF-IDF VECTORS

rf_TF = RandomForestClassifier()
rf_TF.fit(x_train_tf,y_train)

rf_pred_tf = rf_TF.predict(x_test_tf)
accuracy_score(rf_pred_tf, y_test)

# PREDICTION USING DOC2VEC EMBEDDINGS

rf_d2v = RandomForestClassifier()
rf_d2v.fit(x_train_d2v, y_train_xg)

rf_pred_d2v = rf_d2v.predict(x_test_d2v)

accuracy_score(y_test_xg,rf_pred_d2v )

"""**2) FITTING A XGBOOST CLASSIFIER**"""

from xgboost import XGBClassifier

# XGB ON WORD2VEC

xgb_model_w2v = XGBClassifier(max_depth=3, n_estimators=500).fit(x_train_w2v, y_train_xg)
xgb_pred_w2v = xgb_model_w2v.predict(x_test_w2v)

accuracy_score(y_test_xg,xgb_pred_w2v)

# XBG ON TF-IDF vectors

xgb_model_tf = XGBClassifier(max_depth=3, n_estimators=500).fit(x_train_tf, y_train_xg)
xgb_pred_tf = xgb_model_tf.predict(x_test_tf)

accuracy_score(y_test_xg,xgb_pred_tf)

"""# **Deep Learning Model: BiLSTM (Bidirectional LSTM)**"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout
from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier

#BiLSTM Model (Deep Learning)
vocab_size = 20000   # adjust to your tokenizer
embedding_dim = 100 # Adjusted to match Word2Vec vector size
max_length = x_train_w2v.shape[1] # Adjusted to match Word2Vec vector size

bilstm_model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),
    Bidirectional(LSTM(128, return_sequences=False)),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])

bilstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

history = bilstm_model.fit(
    x_train_w2v, y_train_xg,
    validation_data=(x_test_w2v, y_test_xg),
    epochs=5,
    batch_size=64,
    verbose=1
)

# Evaluate
loss, acc_bilstm = bilstm_model.evaluate(x_test_w2v, y_test_xg, verbose=0)
print(f"BiLSTM Model Accuracy: {acc_bilstm:.4f}")

# -------------------------------
# ðŸ“Š Summary
# -------------------------------
print("\n--- Model Comparison ---")
print(f"XGBoost (Word2Vec): {accuracy_score(y_test_xg,xgb_pred_w2v):.4f}")
print(f"XGBoost (TF-IDF):   {accuracy_score(y_test_xg,xgb_pred_tf):.4f}")
print(f"BiLSTM (Deep Learning): {acc_bilstm:.4f}")

from tensorflow.keras.utils import to_categorical

# Assuming 'subreddit' has more than two unique values for multiclass classification.
# If not, this needs adjustment based on the actual classes.
# For this example, assuming there are more than 2 classes in original 'subreddit' column.
# If 'subreddit' only has 'humor' and 'news', then it's binary classification.
# Let's re-check the unique values in the original 'subreddit' column
print(data['subreddit'].unique())

# If there are more than 2 unique values, proceed with one-hot encoding
# If only 'humor' and 'news', then the previous binary setup was correct.
# Assuming the original data might have more classes not shown in the head() output.
# For demonstration of multiclass change:
# Let's create dummy multiclass labels for demonstration purposes if the original is binary.
# If original has more, replace this dummy creation with actual one-hot encoding of original labels.

# As the data.head() shows only 'humor' and 'news', it's a binary classification problem.
# To demonstrate multiclass, let's assume we had 3 classes originally.
# If you have more than 2 classes in your actual data, you should use the original 'y' column.

# If your original problem IS binary, and you want to try a multiclass approach anyway (e.g., exploring different architectures),
# you would still use y_train_xg and y_test_xg but with adjustments to the model output (2 neurons in final layer)
# and loss function (categorical crossentropy), and convert your binary labels to one-hot with 2 columns.

# Let's proceed assuming we need to adapt the existing binary labels (0 and 1) to a categorical format with 2 classes
# because the goal is to change to a model setup typically used for multiclass, even if the data is binary.
# This means the final layer will have 2 neurons and use softmax.
num_classes = 2 # Since the data is binary (0 and 1)

# Convert binary labels to one-hot encoding (for 2 classes)
y_train_categorical = to_categorical(y_train_xg, num_classes=num_classes)
y_test_categorical = to_categorical(y_test_xg, num_classes=num_classes)

# Modify the BiLSTM model for multiclass classification
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout

vocab_size = 20000   # adjust to your tokenizer
embedding_dim = 100 # Adjusted to match Word2Vec vector size
max_length = x_train_w2v.shape[1] # Adjusted to match Word2Vec vector size
num_classes = 2 # Still 2 classes based on your data

bilstm_model_multi = Sequential([
    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),
    Bidirectional(LSTM(128, return_sequences=False)),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(num_classes, activation='softmax') # Change to num_classes and softmax
])

# Change compile loss to categorical_crossentropy
bilstm_model_multi.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the modified model
history_multi = bilstm_model_multi.fit(
    x_train_w2v, y_train_categorical, # Use categorical labels
    validation_data=(x_test_w2v, y_test_categorical), # Use categorical labels
    epochs=5,
    batch_size=64,
    verbose=1
)

# Evaluate the multiclass model
loss_multi, acc_bilstm_multi = bilstm_model_multi.evaluate(x_test_w2v, y_test_categorical, verbose=0)
print(f"BiLSTM Multiclass Model Accuracy: {acc_bilstm_multi:.4f}")

# -------------------------------
# ðŸ“Š Summary with Multiclass Model
# -------------------------------
print("\n--- Model Comparison ---")
print(f"XGBoost (Word2Vec): {accuracy_score(y_test_xg,xgb_pred_w2v):.4f}")
print(f"XGBoost (TF-IDF):   {accuracy_score(y_test_xg,xgb_pred_tf):.4f}")
print(f"BiLSTM Binary Model Accuracy (from previous run): {acc_bilstm:.4f}")
print(f"BiLSTM Multiclass Model Accuracy: {acc_bilstm_multi:.4f}")

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import accuracy_score

# Define parameters
vocab_size = 20000
embedding_dim = 100
max_length = x_train_w2v.shape[1]

# Build simpler BiLSTM model
bilstm_model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),
    Bidirectional(LSTM(64, return_sequences=False)),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Compile with lower learning rate
bilstm_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])

# Early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Train
history = bilstm_model.fit(
    x_train_w2v, y_train_xg,
    validation_data=(x_test_w2v, y_test_xg),
    epochs=20,
    batch_size=32,
    verbose=1,
    callbacks=[early_stopping]
)

# Evaluate
loss, acc_bilstm = bilstm_model.evaluate(x_test_w2v, y_test_xg, verbose=0)
print(f"BiLSTM Model Accuracy: {acc_bilstm:.4f}")

# Manual prediction check
y_pred = (bilstm_model.predict(x_test_w2v) > 0.5).astype(int)
print(f"Manual Binary Accuracy: {accuracy_score(y_test_xg, y_pred):.4f}")

# Plot training history
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'], label='train_accuracy')
plt.plot(history.history['val_accuracy'], label='val_accuracy')
plt.plot(history.history['loss'], label='train_loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.legend()
plt.show()

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# Debug data
print("Training data shape:", x_train_w2v.shape)
print("Test data shape:", x_test_w2v.shape)
print("Training labels unique:", np.unique(y_train_xg, return_counts=True))
print("Test labels unique:", np.unique(y_test_xg, return_counts=True))
print("Sample training data:", x_train_w2v[:2])
print("Sample test data:", x_test_w2v[:2])
print("Sample training labels:", y_train_xg[:10])
print("Sample test labels:", y_test_xg[:10])

# Define parameters
vocab_size = 20000
embedding_dim = 100
max_length = x_train_w2v.shape[1]

# Build BiLSTM model
bilstm_model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=embedding_dim),
    Bidirectional(LSTM(64, return_sequences=False)),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Compile with lower learning rate
bilstm_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])

# Early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Handle class imbalance
from sklearn.utils import class_weight
class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train_xg), y=y_train_xg)
class_weights = dict(enumerate(class_weights))

# Train
history = bilstm_model.fit(
    x_train_w2v, y_train_xg,
    validation_data=(x_test_w2v, y_test_xg),
    epochs=20,
    batch_size=32,
    verbose=1,
    callbacks=[early_stopping],
    class_weight=class_weights
)

# Evaluate
loss, acc_bilstm = bilstm_model.evaluate(x_test_w2v, y_test_xg, verbose=0)
print(f"BiLSTM Model Accuracy: {acc_bilstm:.4f}")

# Manual prediction check
y_pred_raw = bilstm_model.predict(x_test_w2v)
y_pred = (y_pred_raw > 0.5).astype(int)
print("Raw Predictions (first 10):", y_pred_raw[:10])
print("Thresholded Predictions (first 10):", y_pred[:10])
print("True Labels (first 10):", y_test_xg[:10])
print(f"Manual Binary Accuracy: {accuracy_score(y_test_xg, y_pred):.4f}")

# Plot training history
plt.plot(history.history['accuracy'], label='train_accuracy')
plt.plot(history.history['val_accuracy'], label='val_accuracy')
plt.plot(history.history['loss'], label='train_loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.legend()
plt.show()